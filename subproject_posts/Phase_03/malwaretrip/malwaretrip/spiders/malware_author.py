import scrapy
import sys
reload(sys)
sys.setdefaultencoding('UTF8')
sys.path.append('/home/epictions/tls_scripts/tls_utils')
import tls_utils as utils
from scrapy.selector import Selector
from scrapy.http import Request
import datetime
import time
import re
from datetime import timedelta
import MySQLdb
from scrapy.http import Request, FormRequest
import json
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals
import logging
from elasticsearch import Elasticsearch
import hashlib
upsert_query_authors = utils.generate_upsert_query_authors('malwaretrip')

class MalwarePost(scrapy.Spider):
   name ="malware_author"
   start_urls = ["https://malwaretips.com/"]
 
   def __init__(self):
       self.es = Elasticsearch(['10.2.0.90:9342'])
       self.conn = MySQLdb.connect(db= "posts", host = "127.0.0.1", user="tls_dev", passwd = "hdrn!", use_unicode=True, charset="utf8mb4")
       self.cursor = self.conn.cursor()
       select_query = 'select DISTINCT(links) from malware_authors_crawl;'
       self.cursor.execute(select_query)
       self.data = self.cursor.fetchall()

   def mysql_conn_close(self, spider):
       self.conn.commit()
       self.conn.close()

   def close_conn(self, spider):
       self.conn.commit()
       self.conn.close()
   
   def parse(self,response):
       sel = Selector(response)
       headers = {
    'authority': 'malwaretips.com',
    'cache-control': 'max-age=0',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) snap Chromium/80.0.3987.132 Chrome/80.0.3987.132 Safari/537.36',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'referer': 'https://malwaretips.com/',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
     'cookie': '__cfduid=d4078a1c64c715a5aee6a803c814dd9a31582177294; _ga=GA1.2.1873782613.1582177297; xf_notice_dismiss=-1; xf_push_notice_dismiss=1; _gid=GA1.2.494212730.1584073607; _gat_gtag_UA_21403860_1=1; xf_csrf=XrwtlHxi2pRkhdae; xf_user=86124%2Czm2coWbwQ5KZ9RxblRSgPnAQl32rYWWhFlZDjqaS; xf_session=T7gXUqtihTjN5vc8CApOG6TVMFpaqYHY',
}
       yield FormRequest('https://malwaretips.com/', callback=self.parse_next, headers=headers)

    
   def parse_next(self,response):
       urls = []
       for da in self.data:
           urls.append(da[0])
       for url in urls:
           meta_query = 'select DISTINCT(auth_meta) from malware_authors_crawl where links = "%s"'%url
           self.cursor.execute(meta_query)
           self.conn.commit()
           meta_query = self.cursor.fetchall()
           activetime = []
           for da1 in meta_query:
               meta = json.loads(da1[0])
               activetime.append(meta.get('publish_epoch'))
           publish_epoch = set(activetime)
           meta = {'publish_epoch':publish_epoch}
           if url and meta:
               headers = response.request.headers
               if url == 'Null':
                   pass
               else:
                   headers = response.request.headers
                   yield Request(url, callback=self.parse_nxt,headers=headers,meta = meta)

   def parse_nxt(self, response):
       sel = Selector(response)
       user_name = ''.join(response.xpath('//h1[@class="memberHeader-name"]//span[@class="username "]//text()').extract()) 
       authorsignature = ''
       join_date = ''.join(response.xpath('//dl[@class="pairs pairs--inline"]//dt[contains(text(),"Joined")]//..//dd//time//@data-time').extract()).strip()
       last_active = ''.join(response.xpath('//dl[@class="pairs pairs--inline"]//dt[contains(text(),"Last seen")]//..//dd//time//@data-time').extract()).strip() or '0'
       total_posts = ''.join(response.xpath('//dl[@class="pairs pairs--rows pairs--rows--centered fauxBlockLink"]//dt[contains(text(),"Messages")]//..//dd//text()').extract()).strip()
       fetch_time = int(datetime.datetime.now().strftime("%s")) * 1000
       groups = ''.join(response.xpath('//div[@class="memberHeader-blurb"]//span[@class="userTitle"]//text()').extract())
       activetime_ = response.meta.get("publish_epoch")
       activetime = []
       awards = ''.join(response.xpath('//dl[@class="pairs pairs--rows pairs--rows--centered fauxBlockLink"]//dt[@title="Trophy points"]//..//dd//text()').extract()).strip()
       rank = ''.join(response.xpath('//dl[@class="pairs pairs--rows pairs--rows--centered"]//dd//text()').extract()).strip()
       for activetime_i in activetime_:
           try:
               dt = time.gmtime(int(activetime_i)/1000)
               activetime_i = """[ { "year": "%d","month": "%d", "dayofweek": "%d", "hour": "%d", "count": "%s" }]"""%\
                      (dt.tm_year,dt.tm_mon,dt.tm_wday,dt.tm_hour,total_posts)
               activetime.append(activetime_i)
           except:
               activetime.append('-')
       headers = response.request.headers
       url_ = response.xpath('//h2[@class="block-tabHeader block-tabHeader--memberTabs tabs hScroller"]//span//a//@href').extract()[-1]
       url = "https://malwaretips.com" + url_
       meta = {'user_name':user_name,'join_date':join_date,'total_posts':total_posts,'groups':groups,'last_active':last_active,'activetimes':activetime,'awards':awards,'fetch_time':fetch_time,'rank':rank}
       yield Request(url,callback = self.get_meta,headers=headers,meta =meta)

   def get_meta(self,response):
       json_posts = {}
       domain = 'malwaretips.com'
       user_name = response.meta.get('user_name')
       join_date = response.meta.get('join_date')
       total_posts = response.meta.get('total_posts')
       groups = response.meta.get('groups')
       last_active = response.meta.get('last_active')
       activetimes = response.meta.get('activetimes')
       awards = response.meta.get('awards')
       rank = response.meta.get('rank')
       fetch_time = response.meta.get('fetch_time')
       contact_info = []
       website = ''.join(response.xpath('//dl[@class="pairs pairs--columns pairs--fixedSmall"]//dt[contains(text(),"Website")]//../dd//a//text()').extract()).strip()
       if website:
           contact_info.append({'user_id':website,'channel':'WEBSITE'})
       reference_url = response.url
       json_posts.update({'username': user_name,
                          'domain': domain,
                          'auth_sign':'',
                          'join_date': join_date,
                          'lastactive': last_active,
                          'totalposts': total_posts,
                          'fetch_time': fetch_time,
                          'groups': groups,
                          'reputation': '',
                          'credits': '',
                          'awards': awards,
                          'rank': rank,
                          'activetimes': (''.join(activetimes)),
                          'contact_info': str(contact_info)
        })
       self.es.index(index="forum_author", doc_type='post', id=hashlib.md5(str(user_name)).hexdigest(), body=json_posts)
