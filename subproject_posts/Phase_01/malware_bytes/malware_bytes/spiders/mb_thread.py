import mb_xpaths
import scrapy
import datetime
#import sys
#reload(sys)
#sys.setdefaultencoding('UTF8')
import time
import MySQLdb
import json
import re
import selenium
from selenium import webdriver
from scrapy.selector import Selector
from pyvirtualdisplay import Display
import utils
import mb_xpaths

conn=MySQLdb.connect(db="posts_mb",host="localhost",user="root",passwd="",use_unicode=True,charset="utf8mb4")
cursor=conn.cursor()
query = utils.generate_upsert_query_posts('posts_mb')
crawl_query = utils.generate_upsert_query_crawl('posts_mb')

def parse(display, driver):
    select_que = "select distinct(url) from mb_threads_crawl where status_code = 0"
    cursor.execute(select_que)
    data = cursor.fetchall()
    meta = {'Crawl_type':'keep up'}
    for url in data:
        url = url[0]
        up_que = 'update mb_threads_crawl set status_code = 0 where url = "%s"'%url
        cursor.execute(up_que)
        parse_meta(display, driver, url)

def clean_url(response):
    cleaned_url = re.sub(r'(\/\?|\/)$' , '', response)
    return cleaned_url

def parse_meta(display, driver,i):
    json_posts = {}
    driver.get(i)
    time.sleep(1)
    reference_url =  i.encode('utf8')
    sel=Selector(text = driver.page_source)
    if '/?page=' in driver.current_url:
        Crawl_type = "catchup"
        test = re.findall('/?page=\d+',driver.current_url)
        ThreadUrl = driver.current_url.replace(''.join(test),"").replace('?tab=comments','')
        ThreadUrl = clean_url(ThreadUrl)
    else:
        Crawl_type = "keepup"
        ThreadUrl = driver.current_url.replace('?tab=comments','')
        ThreadUrl = clean_url(ThreadUrl)
    domain = "forums.malwarebytes.com"
    category = ''.join(set(sel.xpath(mb_xpaths.categories).extract())).replace('\n','').replace('\t','')
    subcategory = '["' + ''.join(set(sel.xpath(mb_xpaths.subcategories).extract())).replace('\n','').replace('\t','').encode('utf8') + '"]'
    threadtitle = ''.join(sel.xpath(mb_xpaths.threadtitles).extract())
    json_posts.update({'domain': domain,
                      'crawl_type': Crawl_type,
                      'thread_url': ThreadUrl,
                      'category': MySQLdb.escape_string(category),
                      'sub_category': subcategory,
                      'thread_title': threadtitle,
                      'post_title': 'Null',
                      })
    nodes = sel.xpath(mb_xpaths.node_xpaths)
    all_links = []
    for node in nodes:
        authorlink = ''.join(node.xpath(mb_xpaths.author_links).extract()).encode('ascii','ignore')
        posturl = ','.join(node.xpath(mb_xpaths.posturls).extract()).split(',')[1].strip()
        postid  = posturl.split('=')[-1]
        publish = ''.join(node.xpath(mb_xpaths.publishdates).extract())
        try:
            PublishTime = (int(time.mktime(time.strptime(publish, '%m/%d/%Y %H:%M  %p'))) - time.timezone)*1000
        except:
            publishdate = datetime.datetime.strptime(str(publish), '%m/%d/%Y %H:%M  %p')
            PublishTime = time.mktime(publishdate.timetuple())*1000
        fetchtime = round(time.time()*1000)
        author = ''.join(node.xpath(mb_xpaths.author_xpath).extract()).strip().replace('\n','')
        
        
        text_in_list = node.xpath(mb_xpaths.text_xpath).extract()
        text = []
        for txt in text_in_list:
            if ("ware/emoticons/" not in txt ) and (".tinypic.com" not in txt ) :
                text.append(txt)
        text = '\n'.join(text)
        iframe_link = ''.join(node.xpath('.//iframe[@class="ipsEmbed_finishedLoading"]/@src').extract())
        if iframe_link and "youtube.com" not in iframe_link:
            iframe_text = iframe(display, driver,iframe_link)
            text = text.replace(iframe_link,iframe_text)

        text = text.replace('...https://','https://').replace(' //content.','https://content.').replace("ipsQuote","Quote ")
        
        link  = node.xpath(mb_xpaths.urls).extract()
        links = []
        for link_ in set(link):
            if 'emoticons' in link_ or "tinypic" in link_ or '#' == link_ :pass
            else:
                if 'http' not in link_:link_ = 'https:'+ link_
                links.append(link_.encode('utf8'))
        links = '"'+str(links)+'"'
        
        json_posts.update({
                'author_url': authorlink,
                'all_links': links,
                'post_id': postid,
                'post_url':posturl,
                'post_title': 'None',
                'publish_epoch': PublishTime,
                'fetch_epoch': fetchtime,
                'author': author,
                'post_text': utils.clean_spchar_in_text(text),
                'reference_url': reference_url
            })
        cursor.execute(query,json_posts)
        conn.commit()
        meta = json.dumps({
                'time':PublishTime,
                'Threadtitle': threadtitle
                })
        if authorlink:
            json_crawl = {
                    'post_id': postid,
                    'auth_meta': meta,
                    'links': authorlink
                }
            cursor.execute(crawl_query,json_crawl)
            conn.commit()

    links_nagivation = sel.xpath(mb_xpaths.urls_navigation).extract()
    for i in set(links_nagivation):
        if i:
            parse_meta(display, driver,i)


def iframe(display, driver,iframe_link):
    driver.get(iframe_link)
    time.sleep(2)
    res_sel=Selector(text = driver.page_source)
    x = ' '.join(res_sel.xpath('//body[@data-role="internalEmbed"]//div[@class="ipsRichEmbed"]//text() | //img[@class="ipsHide"]//@alt ').extract()).replace('\n','').replace('\t','').replace('\r','')
    try:
        text = ''+x[:380]+'...'+x[-9:]
    except:
        text = x 
    return text

def open_driver():
    display = Display(visible=0, size=(800,600))
    display.start()
    options = webdriver.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument("--disable-extensions")
    options.add_argument('--headless')
    driver = webdriver.Chrome(chrome_options=options)
    return display, driver

def close_driver(display, driver):
    try:
        display.stop()
        driver.quit()
    except Exception as exe:
        process_logger.debug(str(traceback.format_exc()))
        process_logger.debug("Exception while closing driver.")
        pass

if __name__ == "__main__":
    display, driver = open_driver()
    parse(display, driver)
    close_driver(display, driver)
    conn.close()


