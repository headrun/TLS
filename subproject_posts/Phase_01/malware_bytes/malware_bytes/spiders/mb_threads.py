import datetime
import time
import MySQLdb
import json
import re
import selenium
from selenium import webdriver
from scrapy.selector import Selector
import mb_xpaths
import sys
sys.path.append('/home/epictions/tls_scripts/tls_utils')
import tls_utils as utils
from elasticsearch import Elasticsearch
import hashlib

query = utils.generate_upsert_query_posts('malwarebytes')
crawl_query = utils.generate_upsert_query_authors_crawl('malwarebytes')

class  Malwarebytes():
    def __init__(self):
	self.es = Elasticsearch(['10.2.0.90:9342'])
        self.conn,self.cursor = self.mysql_conn()
        self.count = 0
        self.driver = self.open_driver()
        self.parse()
        self.conn.commit()
        self.conn.close()
        self.close_driver()

    def mysql_conn(self):
        conn=MySQLdb.connect(db="posts_mb",host="localhost",user="root",passwd="",use_unicode=True,charset="utf8mb4")
        cursor=conn.cursor()
        return conn,cursor



    def parse(self):
        len_que = 'select count(*) from mb_threads_crawl'
        self.cursor.execute(len_que)
        self.conn.commit()
        total_links = self.cursor.fetchall()
        total_links = int(total_links[0][0])
        for i in range(1,total_links/500+2):
            select_que = "select distinct(post_url) from mb_threads_crawl where crawl_status = 0 limit {0}, {1}".format((i-1)*500,500)
            self.cursor.execute(select_que)
            self.conn.commit()
            data = self.cursor.fetchall()
            meta = {'Crawl_type':'keep up'}
            for url in data:
                url = url[0]
                self.parse_meta(url)

    def parse_meta(self,i):
        json_posts = {}
        self.driver.get(i)
        time.sleep(1)
        reference_url =  i.encode('utf8')
        sel=Selector(text = self.driver.page_source)
        if '/?page=' in self.driver.current_url:
            Crawl_type = "catchup"
            test = re.findall('/?page=\d+',self.driver.current_url)
            ThreadUrl = self.driver.current_url.replace(''.join(test),"").replace('?tab=comments','')
            ThreadUrl = utils.clean_url(ThreadUrl)
        else:
            Crawl_type = "keepup"
            ThreadUrl = self.driver.current_url.replace('?tab=comments','')
            ThreadUrl = utils.clean_url(ThreadUrl)
        domain = "forums.malwarebytes.com"
        category = ''.join(set(sel.xpath(mb_xpaths.categories).extract())).replace('\n','').replace('\t','')
        subcategory = '["' + ''.join(set(sel.xpath(mb_xpaths.subcategories).extract())).replace('\n','').replace('\t','').encode('utf8') + '"]'
        threadtitle = ''.join(sel.xpath(mb_xpaths.threadtitles).extract())
        json_posts.update({'domain': domain,
                          'thread_url': ThreadUrl,
                          'category': MySQLdb.escape_string(category),
                          'sub_category': subcategory,
                          'thread_title': threadtitle,
                          'post_title': '',
                          })

        nodes = sel.xpath(mb_xpaths.node_xpaths)
        all_links = []
        links_nagivation = sel.xpath(mb_xpaths.urls_navigation).extract()
        for i in set(links_nagivation):
            if i:
                try:
                    posturl_ = ','.join(node[-1].xpath(mb_xpaths.posturls).extract()).split(',')[1].strip()
                    test_id = hashlib.md5(str(posturl_)).hexdigest()
	            query = {'query_string': {'use_dis_max': 'true', 'query': '_id:{0}'.format(test_id)}}
		    res = self.es.search(index="forum_posts", body={"query": query})
		    if res['hits']['hits']==[]:
                        self.parse_meta(i)
		except:
		    pass

        for node in nodes:
            authorlink = ''.join(node.xpath(mb_xpaths.author_links).extract()).encode('ascii','ignore')
            posturl = ','.join(node.xpath(mb_xpaths.posturls).extract()).split(',')[1].strip()
            postid  = posturl.split('=')[-1]
            publish = ''.join(node.xpath(mb_xpaths.publishdates).extract())
            try:
                PublishTime = (int(time.mktime(time.strptime(publish, '%m/%d/%Y %H:%M  %p'))) - time.timezone)*1000
            except:
                publishdate = datetime.datetime.strptime(str(publish), '%m/%d/%Y %H:%M  %p')
                PublishTime = time.mktime(publishdate.timetuple())*1000
            fetchtime = round(time.time()*1000)
            author = ''.join(node.xpath(mb_xpaths.author_xpath).extract()).strip().replace('\n','')

            text_in_list = node.xpath(mb_xpaths.text_xpath).extract()
            text = []
            for txt in text_in_list:
                if ("ware/emoticons/" not in txt ) and (".tinypic.com" not in txt ) :
                    text.append(txt)
            text = '\n'.join(text)
            iframe_link = ''.join(node.xpath('.//iframe[@class="ipsEmbed_finishedLoading"]/@src').extract())
            if iframe_link and "youtube.com" not in iframe_link:
                iframe_text = self.iframe(iframe_link)
                text = text.replace(iframe_link,iframe_text)

            text = text.replace('...https://','https://').replace(' //content.','https://content.').replace("ipsQuote","Quote ")

            link  = node.xpath(mb_xpaths.urls).extract()
            links = []
            for link_ in set(link):
                if "tinypic" in link_ or '#' == link_ :pass
                else:
                    if 'http' not in link_:link_ = 'https:'+ link_
                    if 'emoticons' in link_ and '.png' in link_:
                        links.append(link_.encode('utf8'))
                    if 'emoticons' not in link_:
                        links.append(link_.encode('utf8'))
            links = '"'+str(links)+'"'


            json_posts.update({
                    'author_url': authorlink,
                    'links': links,
                    'post_id': postid,
                    'post_url':posturl,
                    'post_title': '',
                    'publish_time': PublishTime,
                    'fetch_time': fetchtime,
                    'author': author,
                    'text': utils.clean_text(text)
                })
	    query={"query":{"match":{"_id":hashlib.md5(str(posturl)).hexdigest()}}}
            res = self.es.search(body=query)
            if res['hits']['hits'] == []:
                self.es.index(index="forum_posts", doc_type='post', id=hashlib.md5(str(posturl)).hexdigest(), body=json_posts)
            meta = json.dumps({
                    'time':PublishTime,
                    'Threadtitle': threadtitle
                    })
            if authorlink:
                json_crawl = {
                        'post_id': postid,
                        'auth_meta': meta,
                        'links': authorlink,
			'crawl_status':0
                    }
                try:
                    self.cursor.execute(crawl_query,json_crawl)
                    self.conn.commit()
                except MySQLdb.OperationalError as e:
                    if 'MySQL server has gone away' in str(e):
                        self.conn,self.cursor = self.mysql_conn()
                        self.cursor.execute(crawl_query,json_crawl)
                        self.conn.commit()
                    else:raise e()

    def iframe(self,iframe_link):
        self.driver.get(iframe_link)
        time.sleep(2)
        res_sel=Selector(text = self.driver.page_source)
        x = ' '.join(res_sel.xpath('//body[@data-role="internalEmbed"]//div[@class="ipsRichEmbed"]//text() | //img[@class="ipsHide"]//@alt ').extract()).replace('\n','').replace('\t','').replace('\r','')
        try:
            text = ''+x[:380]+'...'+x[-9:]
        except:
            text = x
        return text


    def open_driver(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument("--disable-extensions")
        options.add_argument('--headless')
        driver = webdriver.Chrome(chrome_options=options)
        return driver


    def close_driver(self):
        try:
            self.driver.quit()
        except Exception as exe:
            raise exe()


if __name__ == '__main__':
    obj = Malwarebytes()
    
