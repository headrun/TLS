import scrapy
from scrapy.spider import Spider
from scrapy.selector import Selector
from scrapy.http import Request
import MySQLdb
from scrapy import signals
from scrapy.xlib.pydispatch import dispatcher
from scrapy.selector import Selector
import mb_xpaths
import utils
import re 


class formus(Spider):
    name="mb_threads_crawl"
    start_urls = ["https://forums.malwarebytes.com"]


    def __init__(self):
	self.conn,self.cursor = self.mysql_conn()
	dispatcher.connect(self.close_conn, signals.spider_closed)

    def mysql_conn(self):
        conn = MySQLdb.connect(db="posts_mb",host="localhost",user="root",passwd="",use_unicode=True,charset="utf8")
        cursor = conn.cursor()
	return conn, cursor

    def close_conn(self, spider):
        self.conn.commit()
        self.conn.close()

    def parse(self,response):
        sel=Selector(response)
        url = sel.xpath(mb_xpaths.threadurls).extract()
        for i in url:
            if '8-general/' in i:
                yield Request(i,callback=self.parse_general)

    def parse_general(self,response):
        sel=Selector(response)
        meta = {'crawl_type':'keep up'}
        nextpage = sel.xpath(mb_xpaths.nextpage_urls).extract()
        for nextpages in nextpage:
	    nextpages = "https://forums.malwarebytes.com/forum/35-general-chat/"
            yield Request(nextpages,callback=self.parse_generalnextpage,meta= meta)


    def parse_generalnextpage(self,response):
         sel=Selector(response)
         crawl_type = response.meta.get('crawl_type','')
         titlelinks = sel.xpath(mb_xpaths.title_urls).extract()
         for titlelink in titlelinks:
	     if '?page=' in titlelink:
		 continue
             sk = ''.join(re.findall('topic/\d+-',titlelink)).replace('topic/','').replace('-','')
             mb_crawl_que = 'insert into mb_threads_crawl(sk,url,status_code,crawl_type,refrence_url)values(%s,%s,%s ,%s,%s)ON DUPLICATE KEY UPDATE status_code = %s, crawl_type = %s,url = %s, refrence_url = %s'
             val = (sk,titlelink,0,crawl_type,response.url,0,crawl_type,titlelink,response.url)
             try:
                 self.cursor.execute(mb_crawl_que,val)
	     except OperationalError as e:
                if 'MySQL server has gone away' in str(e):
                    self.conn,self.cursor = self.mysql_conn()
		    self.cursor.execute(mb_crawl_que,val)
		else:raise e()
         post_url = response.xpath(mb_xpaths.post_urls).extract()
         for i in set(post_url):
            if i:
                meta = {'crawl_type':'catch up'}
                yield Request(i,callback=self.parse_generalnextpage,meta= meta)

