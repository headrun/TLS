import scrapy
from scrapy.spider import Spider
from scrapy.selector import Selector
from scrapy.http import Request
import datetime
import time
import MySQLdb
import json
import mb_xpaths
import utils
from scrapy import signals
from scrapy.xlib.pydispatch import dispatcher
que = utils.generate_upsert_query_authors('posts_mb')

class formus(scrapy.Spider):
   name="malwarebytes_author"
   start_urls=["https://forums.malwarebytes.com"]

   def __init__(self,*args,**kwargs):
       self.conn = MySQLdb.connect(db="posts_mb",host="localhost",user="root",passwd="",use_unicode=True,charset="utf8")
       self.cursor = self.conn.cursor()
       select_query = 'select DISTINCT(links) from mb_crawl'
       self.cursor.execute(select_query)
       self.data = self.cursor.fetchall()
       dispatcher.connect(self.close_conn, signals.spider_closed)

   def close_conn(self, spider):
        self.conn.commit()
        self.conn.close()

   def parse(self,response):
       urls = []
       for da in self.data:
           urls.append(da[0])
       for url in urls:
           meta_query = 'select DISTINCT(auth_meta) from mb_crawl where links = "%s"'%url.encode('utf8')
           self.cursor.execute(meta_query)
           meta_query = self.cursor.fetchall()
           activetime=[]
           Threadtitle = []
           for da1 in meta_query:
               meta = json.loads(da1[0])
               activetime.append(meta.get('time',''))
               Threadtitle.append(meta.get('Threadtitle','').encode('utf8'))
           PublishTime = set(activetime)
           Threadtitle = ', '.join(set(Threadtitle))
           meta = {'PublishTime':PublishTime,'Threadtitle': Threadtitle}
           if url and meta:
               yield Request(url, callback=self.parse_author,meta = meta)

   def parse_author(self, response):
       sel = Selector(response)
       username = ''.join(sel.xpath(mb_xpaths.usernames).extract()).strip().encode('ascii','ignore')
       activetime_ = response.meta.get("PublishTime")
       Threadtitle = response.meta.get('Threadtitle','-')

       activetime = []
       CONTENT_COUNT = ''.join(sel.xpath(mb_xpaths.contentcount).extract()).replace('\t','').replace('\n','').encode('ascii','ignore')
       for activetime_i in activetime_:
           try:
               dt = time.gmtime(int(activetime_i)/1000)
               activetime_i = """[ { "year": "%d","month": "%d", "dayofweek": "%d", "hour": "%d", "count": "%s" }]"""%\
                      (dt.tm_year,dt.tm_mon,dt.tm_wday,dt.tm_hour,CONTENT_COUNT)
               activetime.append(activetime_i)
           except:
               activetime.append('-')
       totalposts =''.join( CONTENT_COUNT)
       join_time = ''.join(sel.xpath(mb_xpaths.jointime).extract()).replace(' ','').encode('ascii','ignore')
       try:
           aditya = datetime.datetime.strptime(join_time, '%m/%d/%Y%I:%M%p')
           join_date = time.mktime(aditya.timetuple())*1000
       except:
           try:
               aditya = datetime.datetime.strptime(join_time, '%d/%m/%Y%I:%M%p')
               join_date = time.mktime(aditya.timetuple())*1000
           except:
               pass

       last = ''.join(sel.xpath(mb_xpaths.lasts).extract()).encode('ascii','ignore')
       try:
           publishdate = datetime.datetime.strptime(last, '%m/%d/%Y %I:%M %p')
           lastactive = time.mktime(publishdate.timetuple())*1000
       except:
           lastactive = 0
       reputations = ''.join(sel.xpath('//div[@class="ipsDataItem_generic ipsType_break"]//span/@class').extract()).count('ipsPip')
       rank=''.join(''.join(sel.xpath(mb_xpaths.ranks).extract())).strip().encode('utf8')
       
       FetchTime = int(datetime.datetime.now().strftime("%s")) * 1000
       try:
           total_posts = int(totalposts.replace(',',''))
       except:
           total_posts = 0
       json_val = {}
       json_val.update({
           'user_name':MySQLdb.escape_string(username),
           'domain':"forums.malwarebytes.com",
           'crawl_type':"keep up", 
           'author_signature':"None", 
           'join_date':join_date, 
           'last_active':lastactive,
           'total_posts': total_posts, 
           'fetch_time':FetchTime, 
           'groups':"None", 
           'reputation':reputations, 
           'credits':"None",
           'awards': "None",
           'rank':rank,
           'active_time': str(', '.join(activetime)) ,
           'contact_info': 'None',
           'reference_url':MySQLdb.escape_string(response.url),
           })


       self.cursor.execute(que,json_val)
       self.conn.commit()


