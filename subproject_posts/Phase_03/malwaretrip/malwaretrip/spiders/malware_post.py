import scrapy
import MySQLdb
from scrapy import Selector
from scrapy.http import Request
from urlparse import urlparse
from urlparse import urljoin
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals
import re
import datetime
import sys
import json
from pprint import pprint
sys.path.append('/home/epictions/tls_scripts/tls_utils')
import tls_utils as utils
import time
import hashlib
from malwaretrip.items import MalwaretripItem
from elasticsearch import Elasticsearch
from urlparse import urljoin
POST_QUERY = utils.generate_upsert_query_posts_crawl('malwaretrip')
crawl_query = utils.generate_upsert_query_authors_crawl('malwaretrip')

class MalwarePost(scrapy.Spider):
    name = "malware_post"

    def __init__(self):
        self.conn = MySQLdb.connect(db='posts',host='127.0.0.1',user='tls_dev',passwd='hdrn!', use_unicode = 'True', charset = 'utf8mb4')
        self.cursor = self.conn.cursor()
        self.es = Elasticsearch('10.2.0.90:9342')
        dispatcher.connect(self.close_conn, signals.spider_closed)

    def close_conn(self, spider):
        self.conn.commit()
        self.conn.close()
    def start_requests(self):
        self.start_urls = self.cursor.execute("select post_url from malware_crawl where crawl_status = 0")
        post_url = self.cursor.fetchall()
        for url in post_url:
            if 'http://' not in url[0]:
                link = urljoin('https://malwaretips.com/',url[0])
            else:
                link = url[0]
            yield Request(link, callback=self.parse_meta_data)

    def parse_meta_data(self,response):
        sel= Selector(response)
        category = response.xpath('//ul[@class="p-breadcrumbs "]//span[@itemprop="name"]/text()')[1].extract()  or 'Null'
        sub_category = str(sel.xpath('//ul[@class="p-breadcrumbs "]//span[@itemprop="name"]/text()').extract()[2])  or 'Null'
	sub_category_url = response.xpath('//a[@itemprop= "item"]//@href')[2].extract() 
	if sub_category_url:
		sub_category_url = urljoin('https://malwaretips.com/forums',sub_category_url)
	else:
	    sub_category_url = 'Null'
        thread_title =  ''.join( response.xpath('//h1[@class="p-title-value"]/text()').extract())  or 'Null'
        thread_url = response.url
        nodes = response.xpath('//div[@class="message-inner"]')
        if nodes:
            query = 'update malware_crawl set crawl_status = 1 where post_url = %(url)s'
            val = {'url':response.request.url}
            self.cursor.execute(query,val)
            self.conn.commit()

        for node in nodes:
            post_url = ''.join(node.xpath('.//ul[@class="message-attribution-opposite message-attribution-opposite--list"]/li/a[@data-xf-init="share-tooltip"]/@href').extract())
            post_url =  'https://malwaretips.com' + post_url  or 'Null'
            post_id = post_url.split("-")[-1]  or 'Null'
	    post_title = '' or 'Null'
	    ord_in_thread = ''.join(node.xpath('.//ul[@class = "message-attribution-opposite message-attribution-opposite--list"]//li//a//text()').extract()).replace('\n','').replace('#','').replace('\t', '').strip() or 'Null'
            publish_time=  ''.join(node.xpath('.//div[@class="message-attribution-main"]//time[@class="u-dt"]/@data-date-string').extract()) 
            try:
                publish = datetime.datetime.strptime(publish_time,'%b %d, %Y')
                publish_epoch = time.mktime(publish.timetuple())*1000
		if publish_epoch:
                    year = time.strftime("%Y", time.localtime(int(publish_epoch/1000)))
                    if year > '2011':
                        month_year = time.strftime("%m_%Y", time.localtime(int(publish_epoch/1000)))
                    else:
                        continue
            except:
		try:
                    publish = datetime.datetime.strptime(publish_time,'%b %d, %Y')
                    publish_epoch = time.mktime(publish.timetuple())*1000
		    if publish_epoch:
                        year = time.strftime("%Y", time.localtime(int(publish_epoch/1000)))
                        if year > '2011':
                    	    month_year = time.strftime("%m_%Y", time.localtime(int(publish_epoch/1000)))
                        else:
                            continue
                except:
                    pass
 	    if publish_time =='':
		publish_time = 'Null'
            fetch_epoch = utils.fetch_time()
            author = ''.join(node.xpath('.//div[@class="uix_messagePostBitWrapper"]//div//h4//text()').extract()) or 'Null'
            authorurl = ' '.join(node.xpath('.//h4[@class="message-name"]//a[@class="username "]//@href').extract()) 
	    if authorurl:
            	author_url = 'https://malwaretips.com' + authorurl 
	    if authorurl == '':
		author_url = 'Null'
            post_text = ''.join(node.xpath('.//div[@class="bbWrapper"]//text() |.//img[@class="smilie"]//@alt | .//blockquote[@class="bbCodeBlock bbCodeBlock--expandable bbCodeBlock--quote"]/@class | .//blockquote[@class="bbCodeBlock bbCodeBlock--expandable bbCodeBlock--quote"]/text() |.//div[@class="bbCodeBlock bbCodeBlock--screenLimited bbCodeBlock--code"]/@class ').extract()).replace('bbCodeBlock bbCodeBlock--expandable bbCodeBlock--quote','Quote').replace('bbCodeBlock bbCodeBlock--screenLimited bbCodeBlock--code','Quote').replace('\n','')
            post_text = utils.clean_text(post_text) or 'Null'
            all_link_ = node.xpath('.//div[@class="bbMediaWraipper-inner"]//iframe//@src | .//div[@class="attachment-icon attachment-icon--img"]/a//@href | .//div[@class="attachment-name"]/a//@href | .//div[@class="attachment-icon attachment-icon--img"]/a//@src | .//img[@class="bbImage"]//@src | .//a[@class="link link--external"]//@href | .//a[@class="link link--internal"]//@href | .//div[@class="bbCodeBlock-expandContent"]//a/@href | .//a[@class="bbCodeBlock-sourceJump"]//@href').extract()
            all_links = []
            for all_link in all_link_:
                all_links.append(urljoin("https://malwaretips.com",all_link))
	    links = ', '.join(all_links)
	    if links =='':
		links = 'Null'
	    if all_link_ == []:
		links = 'Null'

            author_data = {
			'name':author,
			'url':author_url
			}
            doc = { 
		        'record_id' : re.sub(r"\/$", "", post_url.replace(r"https", "http").replace(r"www.", "")),
                        'hostname': 'malwaretips.com',
                        'domain': "malwaretips.com",
                        'sub_type':'openweb',
                        'type' : 'forum',
                        'author':json.dumps(author_data),
                        'title':thread_title,
                        'text': post_text,
                        'url': post_url,
                        'original_url': post_url,
                        'fetch_time': utils.fetch_time(),
                        'publish_time': publish_epoch,
                        'link.url': links,
                        'post':{
                        'cache_link':'',
			'author':json.dumps(author_data),
                        'section':category,
                        'language':'english',
                        'require_login':'false',
                        'sub_section':sub_category,
                        'sub_section_url':sub_category_url,
                        'post_id': post_id,
                        'post_title':post_title,
                        'ord_in_thread': ord_in_thread,
                        'post_url': post_url,
                        'post_text':post_text,
                        'thread_title':thread_title,
                        'thread_url': thread_url
			},
            }
            self.es.index(index='forum_posts_'+month_year,doc_type = 'post',id=hashlib.md5(post_url).hexdigest(),body = doc)
            meta = {'publish_epoch': publish_epoch}
            json_crawl = {
                    'post_id': post_id,
                    'auth_meta': json.dumps(meta),
                    'crawl_status':0,
                    'links': author_url
            }
            self.cursor.execute(crawl_query, json_crawl)
            self.conn.commit()
	navigation =  response.xpath('//div[@class ="block-outer-main"]//a[contains(@class,"pageNav-jump pageNav-jump--next")]/@href').extract_first()
	if navigation:
	    navigation = urljoin('https://malwaretips.com',navigation)
	    yield Request(navigation, callback =self.parse_meta_data)


    
